:BEP: 45
:Title: Multiple-address operation for the BitTorrent DHT
:Version: $Revision$
:Last-Modified: $Date$
:Author:  The 8472 <the8472@infinite-source.de>
:Status:  Draft
:Type:  Informational
:Content-Type: text/x-rst
:Created: 26-Jun-2015
:Post-History: 


Multiple-address operation for the BitTorrent DHT
=================================================

This BEP provides implementation advice for clients that wish to operate a DHT (BEP 5) node on a host that has multiple globally routable unicast addresses.

In large parts it is based based on practical experience operating such a node with 256+ IPv4 and IPv6 addresses.

Nomenclature
============

**socket address**: in this document generally refers to a socket bound to a distinct, specific address and port and thus is used interchangeably with ``<IP:Port>`` pair and the DHT state/behavior associated with it.

**node**: a DHT peer, which utilizes one or more socket addresses. Remote nodes are assumed to be interchangeable with a single socket address on their end.


Motivation
==========

A node operator might choose to run a DHT node on multiple addresses for the purpose of source-specific multi-path routing [#multi-source]_ or for research purposes to sample a larger fraction of DHT traffic.


Node Behavior
=============

In general any DHT node using multiple socket addresses should appear to the outside world as separate, independent DHT nodes. Exceptions to this rule will be outlined in a later section.

This spares other implementations from having to deal with any idiosyncrasies that might otherwise arise from a single node sending from multiple socket addresses.

The easiest approach to achieve this is to simply run multiple independent DHT nodes, each bound to a separate address and, if neeeded, perform separate announces through each of them

Mandatory
'''''''''

The most important constraints are as follows:

Each socket address must have a distinct Node ID.

    **Rationale**: To an external observer shared node IDs would look like a node constantly changing its IPs (e.g. unreliable domestic internet connection) or port (pathological NAT implementations) and thus deem them as unreliable and evict them from their routing table. 

Node IDs should not only be distinct but also vastly separated by XOR distance, i.e. differ in some of their high order bits. This can be most easily achieved by generating a random node ID for each socket address. Alternatively an implementation can derive new IDs from a base ID by incrementing the base ID in reverse bit order, i.e. by flipping the high order bits first. Care should be taken to avoid counter reuse across multiple active sockets.

    **Rationale**: Failure of a multi-homed node which only differ in low-order bits could take a small slice of the keyspace with it into oblivion, thus violating redundancy of the DHT.

Responses must be sent from the same socket address on which the corresponding request was received

Write-tokens must only be sent from the same socket on which they were received

Implementations should avoid querying the same targets from multiple sockets at once as it may cause unnecessary load spikes for remote nodes. This becomes more important as the number of socket addresses increases. Temporally spacing lookups to the same target keys may accomplish this.


Recommended/Optional
''''''''''''''''''''

Nodes should avoid utilizing multiple ports on a single IP address.

    **Rationale**: Other nodes might throttle or implement other sanitizing behavior based on IP.

Stipulations regarding socket binding from BEP 32 apply, i.e. nodes should avoid binding sockets to the "unspecified address" (0.0.0.0 or ::/128).

For IPv4 autoconfiguration an implementation may choose the following strategies:

A) if available bind to all available global unicast addresses
B) if no global unicast address is available determine the local address used for the default route and bind to that and bootstrap in single-address mode.

   Optionally, once bootstrapping has completed and nodes supporting BEP 42 ``ip`` fields have been found they can be used to determine the external address of other available local addresses and then switch to multi-address mode if additional external addresses are available.
  


For IPv6 autoconfiguration nodes should space IPv6 addresses as widely as they can reasonably afford.

* if multiple prefix routes are available then socket address should utilize at least one socket address from each
* alternatively, if path/routing information can be obtained an implementation should utilize at least one address per path
* if multiple addresses within a single larger prefix are available it should avoid picking more than one per /64
* if multiple choices per network interface are available the current address generated by IPv6 privacy extensions (RFC 4941) is preferable.

..

    **Rationale**: Other nodes might treat IPv6 /64 subnets as a single entity for the purpose of spam / malicious behavior detection and thus impose limits on the services they offer to nodes within such a block or how many entries they permit in their routing table
    

Ultimately the exact network configuration of a multi-homed setup is hard to predict, therefore a user should be able to explicitly configure which network interfaces/prefixes/socket addresses a node should bind to.
Documentation mirroring these guidelines may also aid the user in configuring his network.



Implementation Guidance
=======================

Determining Connectivity
''''''''''''''''''''''''

Implementations may want to apply some heuristic based on responses and incoming unsolicited requests to determine whether individual socket addresses are reachable.
This information can be used to ignore timeouts which occur on non-connected socket addresses for the purpose of routing table maintenance. In other words routing table entries should not be penalized for timeouts on a link known to be down when they are still reachable over other links.

Additionally nodes may preferentially schedule data lookups on known-good socket addresses while performing maintenance pings on all socket addresses - including bad ones - in the hope that they eventually become reachable again.


Shared Routing Table
'''''''''''''''''''''

BEP 5 describes a fixed-size routing table of 160 buckets which are ordered relative to a Node ID. Since a multi-address node has multiple IDs it will have to use one routing table for each of its socket addresses.

Alternatively a multi-address node may implement a modified version of the variable-size, tree-like routing table, ordered by the natural distance of the buckets, as described in the original `Kademlia`_ paper.
It only has to be modified to consider multiple home buckets instead of - each corresponding to a node ID - on which split operations may be performed instead of one.

A node may use the shared routing table to

* return better node lists to incoming queries
* spread traffic caused by active routing table maintenance over multiple sockets
* slightly accelerate lookups by having a finer-grained routing table due to increased bucket splitting
* to verify that remote nodes' IDs are observer-independent, i.e. to detect a certain type of malicious behavior

  when receiving a valid response, verified through its transaction ID, from a remote node whose *socket address* is found in the routing table but whose ID mismatches the entry in the routing table then this entry may be evicted immediately.

When performing maintenance lookups targeting a local ID (e.g. bootstrap lookups) then those should be performed on the socket address corresponding to that particular target ID. This is necessary to make the presence of this particular socket address known to immediate neighbors in the keyspace.

Other types of maintenance traffic can be distributed freely among socket addresses. 


Shared Storage
''''''''''''''

Storage for incoming ``announce_peer``, ``put`` and similar write requests can be shared among multiple socket addresses with little positive or negative effect.

Multi addresses ``announce_peer``
'''''''''''''''''''''''''''''''''

Since remote nodes may restrict the validity of write tokens to the IP address to which they have been issued, a node cannot simply announce to other nodes from multiple socket addresses. It has to perform new lookups to obtain new tokens for each of them.

But it can reuse the set of responding nodes from a lookup on one socket address as candidate set for a lookup on another socket address, that may significantly accelerate a lookup.

As mentioned earlier individual lookups should be performed with some time-delay between them to avoid hammering the same nodes from multiple sockets.


Multi-address announce using temporary sockets
''''''''''''''''''''''''''''''''''''''''''''''

Instead of permanently operating a DHT in multi-address mode an implementation may choose to mainly operate in single-address mode and only spin up additional UDP sockets to announce from other socket addresses as needed and close them once the announces have been finished.
To avoid polluting the routing tables of other nodes BEP 43 must be used to signal that those temporary sockets are essentially in read-only state.

Unique node IDs still must be generated for each temporary socket and new tokens must be obtained as described in the previous section.

This approach should be applied judiciously. Announcing more than a handful of addresses while only providing the capacity of a single-address node might be seen as abuse.

Another downside is that it does not utilize fault-tolerance in environments where multiple independent routes are available.


Other write operations
''''''''''''''''''''''

Other types of write operations, such as BEP 44 ``put`` may not benefit from being performed on more than one socket address at a given time.

It is therefore sufficient to schedule them on a randomly selected (active) socket address each time they are to be performed.



User Guidance
=============

The traffic a DHT node receives is roughly proportional to its number of socket addresses and also grows with uptime, especially once reaching 24 hours of uptime.
At some point various network components may become overwhelmed by the UDP traffic flowing to potentially millions of different remote addresses and lead to packet drops.

To prevent issues connection tracking should be disabled for any traffic flowing from and to the local ports used by the DHT node. No assumptions about remote ports should be made.
Disabling connection tracking disables automatic NAT, therefore static NAT rules should be setup where necessary.




References
=========

.. [#multi-source] "Source-specific routing", Boutier, Chroboczek
   (http://arxiv.org/pdf/1403.0445.pdf)

.. _Kademlia: http://pdos.csail.mit.edu/~petar/papers/maymounkov-kademlia-lncs.pdf